{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "touched-logic",
   "metadata": {},
   "source": [
    "# Theory/Computation Problems\n",
    "\n",
    "### Problem 1 (20 points) \n",
    "Show that the stationary point (zero gradient) of the function\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    f=2x_{1}^{2} - 4x_1 x_2+ 1.5x^{2}_{2}+ x_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "is a saddle (with indefinite Hessian). Find the directions of downslopes away from the saddle. Hint: Use Taylor's expansion at the saddle point. Find directions that reduce $f$.\n",
    "\n",
    "### Problem 2 (50 points) \n",
    "\n",
    "* (10 points) Find the point in the plane $x_1+2x_2+3x_3=1$ in $\\mathbb{R}^3$ that is nearest to the point $(-1,0,1)^T$. Is this a convex problem? Hint: Convert the problem into an unconstrained problem using $x_1+2x_2+3x_3=1$.\n",
    "\n",
    "* (40 points) Implement the gradient descent and Newton's algorithm for solving the problem. Attach your codes along with a short summary including (1) the initial points tested, (2) corresponding solutions, (3) a log-linear convergence plot.\n",
    "\n",
    "### Problem 3 (10 points) \n",
    "Let $f(x)$ and $g(x)$ be two convex functions defined on the convex set $\\mathcal{X}$. \n",
    "* (5 points) Prove that $af(x)+bg(x)$ is convex for $a>0$ and $b>0$. \n",
    "* (5 points) In what conditions will $f(g(x))$ be convex?\n",
    "\n",
    "### Problem 4 (bonus 10 points)\n",
    "Show that $f({\\bf x}_1) \\geq f(\\textbf{x}_0) + \n",
    "    \\textbf{g}_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0)$ for a convex function $f(\\textbf{x}): \\mathcal{X} \\rightarrow \\mathbb{R}$ and for $\\textbf{x}_0$, $\\textbf{x}_1 \\in \\mathcal{X}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-carbon",
   "metadata": {},
   "source": [
    "# Design Problems\n",
    "\n",
    "### Problem 5 (20 points) \n",
    "Consider an illumination problem: There are $n$ lamps and $m$ mirrors fixed to the ground. The target reflection intensity level is $I_t$. The actual reflection intensity level on the $k$th mirror can be computed as $\\textbf{a}_k^T \\textbf{p}$, where $\\textbf{a}_k$ is given by the distances between all lamps to the mirror, and $\\textbf{p}:=[p_1,...,p_n]^T$ are the power output of the lamps. The objective is to keep the actual intensity levels as close to the target as possible by tuning the power output $\\textbf{p}$.\n",
    "\n",
    "* (5 points) Formulate this problem as an optimization problem. \n",
    "* (5 points) Is your problem convex?\n",
    "* (5 points) If we require the overall power output of any of the $n$ lamps to be less than $p^*$, will the problem have a unique solution?\n",
    "* (5 points) If we require no more than half of the lamps to be switched on, will the problem have a unique solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-twins",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "For this homework, you may want to attach sketches as means to explain your ideas. Here is how you can attach images.\n",
    "\n",
    "![everly1](img/everly7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d616022",
   "metadata": {},
   "source": [
    "[img](01.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b876c6d2",
   "metadata": {},
   "source": [
    "[img](02.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff98041",
   "metadata": {},
   "source": [
    "[img](03.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c05919",
   "metadata": {},
   "source": [
    "[img](04.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8a911",
   "metadata": {},
   "source": [
    "[img](05.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe0d1e7",
   "metadata": {},
   "source": [
    "[img](06.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3a37ca",
   "metadata": {},
   "source": [
    "[img](07.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b075b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "[img]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "moving-prescription",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.25,\n",
       " 0.4375,\n",
       " 0.578125,\n",
       " 0.68359375,\n",
       " 0.7626953125,\n",
       " 0.822021484375,\n",
       " 0.86651611328125,\n",
       " 0.8998870849609375,\n",
       " 0.9249153137207031,\n",
       " 0.9436864852905273,\n",
       " 0.9577648639678955,\n",
       " 0.9683236479759216,\n",
       " 0.9762427359819412,\n",
       " 0.9821820519864559,\n",
       " 0.9866365389898419,\n",
       " 0.9899774042423815,\n",
       " 0.9924830531817861,\n",
       " 0.9943622898863396,\n",
       " 0.9957717174147547,\n",
       " 0.996828788061066,\n",
       " 0.9976215910457995,\n",
       " 0.9982161932843496,\n",
       " 0.9986621449632622,\n",
       " 0.9989966087224467,\n",
       " 0.999247456541835,\n",
       " 0.9994355924063762,\n",
       " 0.9995766943047821]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample code for Problem 2\n",
    "\n",
    "obj = lambda x: (x - 1)**2  # note that this is 1D. In Prob 2 it should be 2D.\n",
    "grad = lambda x: 2*(x - 1)  # this is not the correct gradient!\n",
    "eps = 1e-3  # termination criterion\n",
    "x0 = 0.  # initial guess\n",
    "k = 0  # counter\n",
    "soln = [x0]  # use an array to store the search steps\n",
    "x = soln[k]  # start with the initial guess\n",
    "error = abs(grad(x))  # compute the error. Note you will need to compute the norm for 2D grads, rather than the absolute value\n",
    "# a = 0.01  # set a fixed step size to start with\n",
    "\n",
    "# Armijo line search\n",
    "def line_search(x):\n",
    "    a = 1.  # initialize step size\n",
    "    phi = lambda a, x: obj(x) - a*0.8*grad(x)**2  # define phi as a search criterion\n",
    "    while phi(a,x)<obj(x-a*grad(x)):  # if f(x+a*d)>phi(a) then backtrack. d is the search direction\n",
    "        a = 0.5*a\n",
    "    return a\n",
    "\n",
    "while error >= eps:  # keep searching while gradient norm is larger than eps\n",
    "    a = line_search(x)\n",
    "    x = x - a*grad(x)\n",
    "    soln.append(x)\n",
    "    error = abs(grad(x))\n",
    "    \n",
    "soln  # print the search trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13b4fa74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0],\n",
       " array([0.08, 0.14]),\n",
       " array([0.1352, 0.2424]),\n",
       " array([0.172592, 0.317696]),\n",
       " array([0.19720928, 0.37344576]),\n",
       " array([0.21267486, 0.41509149]),\n",
       " array([0.2215964 , 0.44655221]),\n",
       " array([0.22585049, 0.4706502 ]),\n",
       " array([0.22678742, 0.4894181 ]),\n",
       " array([0.2253785 , 0.50431999]),\n",
       " array([0.22232225, 0.51641057]),\n",
       " array([0.21812076, 0.52644979]),\n",
       " array([0.21313471, 0.53498534]),\n",
       " array([0.207623  , 0.54241211]),\n",
       " array([0.20177124, 0.54901493]),\n",
       " array([0.19571233, 0.55499939]),\n",
       " array([0.18954117, 0.56051403]),\n",
       " array([0.18332537, 0.56566629]),\n",
       " array([0.17711288, 0.57053398]),\n",
       " array([0.17093751, 0.57517364]),\n",
       " array([0.16482292, 0.57962641]),\n",
       " array([0.15878546, 0.58392238]),\n",
       " array([0.15283623, 0.58808365]),\n",
       " array([0.14698257, 0.59212657]),\n",
       " array([0.14122912, 0.59606335]),\n",
       " array([0.13557861, 0.59990318]),\n",
       " array([0.13003237, 0.60365311]),\n",
       " array([0.12459076, 0.60731861]),\n",
       " array([0.11925345, 0.610904  ]),\n",
       " array([0.11401962, 0.61441278]),\n",
       " array([0.10888813, 0.61784787]),\n",
       " array([0.10385757, 0.62121172]),\n",
       " array([0.09892641, 0.62450647]),\n",
       " array([0.09409299, 0.62773401]),\n",
       " array([0.08935561, 0.63089605]),\n",
       " array([0.08471252, 0.63399416]),\n",
       " array([0.08016197, 0.63702983]),\n",
       " array([0.07570219, 0.64000443]),\n",
       " array([0.07133144, 0.64291928]),\n",
       " array([0.06704799, 0.64577565]),\n",
       " array([0.06285011, 0.64857476]),\n",
       " array([0.05873613, 0.6513178 ]),\n",
       " array([0.05470438, 0.6540059 ]),\n",
       " array([0.05075323, 0.6566402 ]),\n",
       " array([0.04688109, 0.65922177]),\n",
       " array([0.04308637, 0.66175168]),\n",
       " array([0.03936753, 0.66423098]),\n",
       " array([0.03572306, 0.66666068]),\n",
       " array([0.03215147, 0.66904178]),\n",
       " array([0.02865131, 0.67137525]),\n",
       " array([0.02522115, 0.67366204]),\n",
       " array([0.02185959, 0.6759031 ]),\n",
       " array([0.01856526, 0.67809933]),\n",
       " array([0.01533681, 0.68025163]),\n",
       " array([0.01217294, 0.68236089]),\n",
       " array([0.00907234, 0.68442796]),\n",
       " array([0.00603375, 0.68645368]),\n",
       " array([0.00305593, 0.6884389 ]),\n",
       " array([1.37669802e-04, 6.90384407e-01]),\n",
       " array([-0.00272223,  0.69229101]),\n",
       " array([-0.00552492,  0.69415947]),\n",
       " array([-0.00827157,  0.69599057]),\n",
       " array([-0.01096328,  0.69778504]),\n",
       " array([-0.01360116,  0.69954363]),\n",
       " array([-0.01618628,  0.70126704]),\n",
       " array([-0.01871969,  0.70295599]),\n",
       " array([-0.02120244,  0.70461115]),\n",
       " array([-0.02363554,  0.70623321]),\n",
       " array([-0.02601997,  0.70782284]),\n",
       " array([-0.02835671,  0.70938067]),\n",
       " array([-0.03064672,  0.71090734]),\n",
       " array([-0.03289093,  0.71240348]),\n",
       " array([-0.03509025,  0.71386969]),\n",
       " array([-0.03724559,  0.71530658]),\n",
       " array([-0.03935782,  0.71671474]),\n",
       " array([-0.04142781,  0.71809473]),\n",
       " array([-0.0434564 ,  0.71944712]),\n",
       " array([-0.04544441,  0.72077246]),\n",
       " array([-0.04739266,  0.7220713 ]),\n",
       " array([-0.04930195,  0.72334416]),\n",
       " array([-0.05117306,  0.72459156]),\n",
       " array([-0.05300674,  0.72581402]),\n",
       " array([-0.05480375,  0.72701202]),\n",
       " array([-0.05656482,  0.72818607]),\n",
       " array([-0.05829066,  0.72933663]),\n",
       " array([-0.05998199,  0.73046419]),\n",
       " array([-0.0616395 ,  0.73156919]),\n",
       " array([-0.06326385,  0.73265209]),\n",
       " array([-0.06485571,  0.73371333]),\n",
       " array([-0.06641574,  0.73475335]),\n",
       " array([-0.06794457,  0.73577257]),\n",
       " array([-0.06944282,  0.73677141]),\n",
       " array([-0.07091111,  0.73775026]),\n",
       " array([-0.07235003,  0.73870954]),\n",
       " array([-0.07376017,  0.73964964]),\n",
       " array([-0.07514211,  0.74057093]),\n",
       " array([-0.07649641,  0.7414738 ]),\n",
       " array([-0.07782363,  0.74235861]),\n",
       " array([-0.0791243 ,  0.74322572]),\n",
       " array([-0.08039895,  0.74407549]),\n",
       " array([-0.08164812,  0.74490827]),\n",
       " array([-0.0828723 ,  0.74572439]),\n",
       " array([-0.08407199,  0.74652419]),\n",
       " array([-0.0852477 ,  0.74730799]),\n",
       " array([-0.08639989,  0.74807611]),\n",
       " array([-0.08752903,  0.74882888]),\n",
       " array([-0.08863559,  0.74956659]),\n",
       " array([-0.08972002,  0.75028954]),\n",
       " array([-0.09078277,  0.75099804]),\n",
       " array([-0.09182425,  0.75169236]),\n",
       " array([-0.09284491,  0.7523728 ]),\n",
       " array([-0.09384516,  0.75303963]),\n",
       " array([-0.0948254 ,  0.75369312]),\n",
       " array([-0.09578603,  0.75433354]),\n",
       " array([-0.09672745,  0.75496116]),\n",
       " array([-0.09765005,  0.75557622]),\n",
       " array([-0.09855419,  0.75617898]),\n",
       " array([-0.09944025,  0.75676969]),\n",
       " array([-0.10030859,  0.75734858]),\n",
       " array([-0.10115956,  0.7579159 ]),\n",
       " array([-0.10199351,  0.75847186]),\n",
       " array([-0.10281078,  0.75901671]),\n",
       " array([-0.10361171,  0.75955066]),\n",
       " array([-0.10439662,  0.76007394]),\n",
       " array([-0.10516583,  0.76058674]),\n",
       " array([-0.10591965,  0.76108929]),\n",
       " array([-0.1066584 ,  0.76158179]),\n",
       " array([-0.10738238,  0.76206444]),\n",
       " array([-0.10809187,  0.76253744]),\n",
       " array([-0.10878718,  0.76300098]),\n",
       " array([-0.10946858,  0.76345524]),\n",
       " array([-0.11013635,  0.76390042]),\n",
       " array([-0.11079077,  0.7643367 ]),\n",
       " array([-0.11143209,  0.76476425]),\n",
       " array([-0.11206059,  0.76518325]),\n",
       " array([-0.11267653,  0.76559387]),\n",
       " array([-0.11328014,  0.76599628]),\n",
       " array([-0.11387168,  0.76639064]),\n",
       " array([-0.11445139,  0.76677712]),\n",
       " array([-0.1150195 ,  0.76715586]),\n",
       " array([-0.11557626,  0.76752703]),\n",
       " array([-0.11612187,  0.76789077]),\n",
       " array([-0.11665658,  0.76824724]),\n",
       " array([-0.11718059,  0.76859658]),\n",
       " array([-0.11769412,  0.76893894]),\n",
       " array([-0.11819738,  0.76927444]),\n",
       " array([-0.11869058,  0.76960324]),\n",
       " array([-0.11917391,  0.76992546]),\n",
       " array([-0.11964757,  0.77024124]),\n",
       " array([-0.12011176,  0.7705507 ]),\n",
       " array([-0.12056667,  0.77085397]),\n",
       " array([-0.12101248,  0.77115118]),\n",
       " array([-0.12144937,  0.77144244]),\n",
       " array([-0.12187753,  0.77172788]),\n",
       " array([-0.12229712,  0.77200761]),\n",
       " array([-0.12270832,  0.77228174]),\n",
       " array([-0.1231113 ,  0.77255039]),\n",
       " array([-0.12350622,  0.77281367]),\n",
       " array([-0.12389323,  0.77307168]),\n",
       " array([-0.12427251,  0.77332453]),\n",
       " array([-0.1246442 ,  0.77357233]),\n",
       " array([-0.12500846,  0.77381517]),\n",
       " array([-0.12536544,  0.77405315]),\n",
       " array([-0.12571527,  0.77428637]),\n",
       " array([-0.12605811,  0.77451493]),\n",
       " array([-0.12639409,  0.77473892]),\n",
       " array([-0.12672335,  0.77495842]),\n",
       " array([-0.12704603,  0.77517354]),\n",
       " array([-0.12736225,  0.77538436]),\n",
       " array([-0.12767215,  0.77559095]),\n",
       " array([-0.12797585,  0.77579342]),\n",
       " array([-0.12827347,  0.77599184]),\n",
       " array([-0.12856515,  0.77618629]),\n",
       " array([-0.12885099,  0.77637685]),\n",
       " array([-0.12913111,  0.7765636 ]),\n",
       " array([-0.12940563,  0.77674661]),\n",
       " array([-0.12967466,  0.77692596]),\n",
       " array([-0.12993831,  0.77710173]),\n",
       " array([-0.13019669,  0.77727398]),\n",
       " array([-0.1304499 ,  0.77744279]),\n",
       " array([-0.13069804,  0.77760822]),\n",
       " array([-0.13094122,  0.77777034]),\n",
       " array([-0.13117954,  0.77792922]),\n",
       " array([-0.13141309,  0.77808492]),\n",
       " array([-0.13164197,  0.77823751]),\n",
       " array([-0.13186628,  0.77838704]),\n",
       " array([-0.13208609,  0.77853359]),\n",
       " array([-0.13230152,  0.7786772 ]),\n",
       " array([-0.13251263,  0.77881794]),\n",
       " array([-0.13271952,  0.77895587]),\n",
       " array([-0.13292227,  0.77909104]),\n",
       " array([-0.13312097,  0.7792235 ]),\n",
       " array([-0.13331569,  0.77935332]),\n",
       " array([-0.13350652,  0.77948054]),\n",
       " array([-0.13369353,  0.77960521]),\n",
       " array([-0.13387681,  0.77972739]),\n",
       " array([-0.13405641,  0.77984713]),\n",
       " array([-0.13423243,  0.77996447]),\n",
       " array([-0.13440492,  0.78007947]),\n",
       " array([-0.13457397,  0.78019217]),\n",
       " array([-0.13473963,  0.78030261]),\n",
       " array([-0.13490198,  0.78041084]),\n",
       " array([-0.13506108,  0.78051691]),\n",
       " array([-0.135217  ,  0.78062086]),\n",
       " array([-0.13536981,  0.78072273]),\n",
       " array([-0.13551955,  0.78082256]),\n",
       " array([-0.13566631,  0.78092039]),\n",
       " array([-0.13581012,  0.78101627]),\n",
       " array([-0.13595106,  0.78111023]),\n",
       " array([-0.13608918,  0.78120231]),\n",
       " array([-0.13622454,  0.78129255]),\n",
       " array([-0.1363572 ,  0.78138099]),\n",
       " array([-0.13648719,  0.78146765]),\n",
       " array([-0.13661459,  0.78155259]),\n",
       " array([-0.13673944,  0.78163582]),\n",
       " array([-0.1368618 ,  0.78171739]),\n",
       " array([-0.1369817 ,  0.78179733]),\n",
       " array([-0.13709921,  0.78187567]),\n",
       " array([-0.13721437,  0.78195244]),\n",
       " array([-0.13732723,  0.78202768]),\n",
       " array([-0.13743783,  0.78210141]),\n",
       " array([-0.13754621,  0.78217367]),\n",
       " array([-0.13765243,  0.78224448]),\n",
       " array([-0.13775653,  0.78231387]),\n",
       " array([-0.13785854,  0.78238188]),\n",
       " array([-0.13795851,  0.78244853]),\n",
       " array([-0.13805648,  0.78251385]),\n",
       " array([-0.1381525 ,  0.78257785]),\n",
       " array([-0.13824659,  0.78264058]),\n",
       " array([-0.1383388 ,  0.78270206]),\n",
       " array([-0.13842917,  0.7827623 ]),\n",
       " array([-0.13851773,  0.78282134]),\n",
       " array([-0.13860451,  0.7828792 ]),\n",
       " array([-0.13868957,  0.7829359 ]),\n",
       " array([-0.13877292,  0.78299147]),\n",
       " array([-0.1388546 ,  0.78304593]),\n",
       " array([-0.13893465,  0.78309929]),\n",
       " array([-0.1390131 ,  0.78315159]),\n",
       " array([-0.13908998,  0.78320285]),\n",
       " array([-0.13916533,  0.78325308]),\n",
       " array([-0.13923916,  0.7833023 ]),\n",
       " array([-0.13931152,  0.78335054]),\n",
       " array([-0.13938244,  0.78339781]),\n",
       " array([-0.13945193,  0.78344414]),\n",
       " array([-0.13952003,  0.78348955]),\n",
       " array([-0.13958678,  0.78353404]),\n",
       " array([-0.13965218,  0.78357765]),\n",
       " array([-0.13971628,  0.78362038]),\n",
       " array([-0.1397791 ,  0.78366226]),\n",
       " array([-0.13984066,  0.7837033 ]),\n",
       " array([-0.13990099,  0.78374352]),\n",
       " array([-0.13996011,  0.78378293]),\n",
       " array([-0.14001805,  0.78382156]),\n",
       " array([-0.14007484,  0.78385941]),\n",
       " array([-0.14013048,  0.78389651]),\n",
       " array([-0.14018502,  0.78393287]),\n",
       " array([-0.14023846,  0.7839685 ]),\n",
       " array([-0.14029083,  0.78400341]),\n",
       " array([-0.14034216,  0.78403763]),\n",
       " array([-0.14039246,  0.78407116]),\n",
       " array([-0.14044175,  0.78410402]),\n",
       " array([-0.14049006,  0.78413623]),\n",
       " array([-0.1405374 ,  0.78416779]),\n",
       " array([-0.1405838 ,  0.78419872]),\n",
       " array([-0.14062926,  0.78422903]),\n",
       " array([-0.14067382,  0.78425874]),\n",
       " array([-0.14071749,  0.78428785]),\n",
       " array([-0.14076028,  0.78431638]),\n",
       " array([-0.14080222,  0.78434434]),\n",
       " array([-0.14084332,  0.78437173]),\n",
       " array([-0.14088359,  0.78439859]),\n",
       " array([-0.14092306,  0.7844249 ]),\n",
       " array([-0.14096174,  0.78445069]),\n",
       " array([-0.14099965,  0.78447596]),\n",
       " array([-0.1410368 ,  0.78450073]),\n",
       " array([-0.14107321,  0.784525  ]),\n",
       " array([-0.14110889,  0.78454878]),\n",
       " array([-0.14114385,  0.78457209]),\n",
       " array([-0.14117812,  0.78459494]),\n",
       " array([-0.1412117 ,  0.78461732]),\n",
       " array([-0.14124461,  0.78463926]),\n",
       " array([-0.14127686,  0.78466076]),\n",
       " array([-0.14130846,  0.78468183]),\n",
       " array([-0.14133944,  0.78470248]),\n",
       " array([-0.14136979,  0.78472272]),\n",
       " array([-0.14139954,  0.78474255]),\n",
       " array([-0.14142869,  0.78476198]),\n",
       " array([-0.14145726,  0.78478103]),\n",
       " array([-0.14148526,  0.7847997 ]),\n",
       " array([-0.1415127 ,  0.78481799]),\n",
       " array([-0.14153958,  0.78483591]),\n",
       " array([-0.14156594,  0.78485348]),\n",
       " array([-0.14159176,  0.7848707 ]),\n",
       " array([-0.14161707,  0.78488757]),\n",
       " array([-0.14164187,  0.7849041 ]),\n",
       " array([-0.14166617,  0.78492031]),\n",
       " array([-0.14168999,  0.78493619]),\n",
       " array([-0.14171334,  0.78495175]),\n",
       " array([-0.14173621,  0.784967  ]),\n",
       " array([-0.14175863,  0.78498194]),\n",
       " array([-0.1417806 ,  0.78499659]),\n",
       " array([-0.14180213,  0.78501095]),\n",
       " array([-0.14182323,  0.78502501]),\n",
       " array([-0.14184391,  0.7850388 ]),\n",
       " array([-0.14186418,  0.78505231]),\n",
       " array([-0.14188403,  0.78506555]),\n",
       " array([-0.1419035 ,  0.78507852]),\n",
       " array([-0.14192257,  0.78509124]),\n",
       " array([-0.14194126,  0.7851037 ]),\n",
       " array([-0.14195958,  0.78511591]),\n",
       " array([-0.14197753,  0.78512788]),\n",
       " array([-0.14199512,  0.78513961]),\n",
       " array([-0.14201236,  0.7851511 ]),\n",
       " array([-0.14202926,  0.78516236]),\n",
       " array([-0.14204582,  0.7851734 ]),\n",
       " array([-0.14206204,  0.78518422]),\n",
       " array([-0.14207794,  0.78519482]),\n",
       " array([-0.14209353,  0.78520521]),\n",
       " array([-0.1421088 ,  0.78521539]),\n",
       " array([-0.14212377,  0.78522537]),\n",
       " array([-0.14213844,  0.78523515]),\n",
       " array([-0.14215281,  0.78524473]),\n",
       " array([-0.1421669 ,  0.78525412]),\n",
       " array([-0.1421807 ,  0.78526332]),\n",
       " array([-0.14219423,  0.78527234]),\n",
       " array([-0.14220749,  0.78528118]),\n",
       " array([-0.14222048,  0.78528984]),\n",
       " array([-0.14223321,  0.78529833]),\n",
       " array([-0.14224569,  0.78530665]),\n",
       " array([-0.14225792,  0.78531481]),\n",
       " array([-0.14226991,  0.78532279]),\n",
       " array([-0.14228165,  0.78533062]),\n",
       " array([-0.14229316,  0.7853383 ]),\n",
       " array([-0.14230444,  0.78534582]),\n",
       " array([-0.14231549,  0.78535319]),\n",
       " array([-0.14232633,  0.78536041]),\n",
       " array([-0.14233694,  0.78536749]),\n",
       " array([-0.14234735,  0.78537442]),\n",
       " array([-0.14235754,  0.78538122]),\n",
       " array([-0.14236754,  0.78538788]),\n",
       " array([-0.14237733,  0.78539441]),\n",
       " array([-0.14238692,  0.78540081]),\n",
       " array([-0.14239633,  0.78540708]),\n",
       " array([-0.14240555,  0.78541322]),\n",
       " array([-0.14241458,  0.78541924]),\n",
       " array([-0.14242343,  0.78542514]),\n",
       " array([-0.1424321 ,  0.78543093]),\n",
       " array([-0.1424406 ,  0.78543659]),\n",
       " array([-0.14244893,  0.78544215])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code without line search\n",
    "import numpy as np\n",
    "obj = lambda x: 5*x[0]**2+10*x[1]**2+(12*x[0]*x[1])-8*x[0]-14*x[1]+5# note that this is 1D. In Prob 2 it should be 2D.\n",
    "def grad(x):\n",
    "     return (10*x[0]+12*x[1]-8), (20*x[1]+12*x[0]-14)\n",
    "eps = 1e-3  # termination criterion  \n",
    "x0= [0.,0.]  # initial guess\n",
    "k = 0  # counter\n",
    "soln = [x0]  # use an array to store the search steps    \n",
    "x = soln[k]  # start with the initial guess\n",
    "error = np.linalg.norm(grad(x))  # compute the error. Note you will need to compute the norm for 2D grads, rather than the absolute value\n",
    "a = 0.01  # set a fixed step size to start with\n",
    "\n",
    "# Armijo line search\n",
    "#def line_search(x):\n",
    " #   a = 1.  # initialize step size\n",
    "  #  phi = lambda a, x: obj(x) - a*0.8*grad(x)**2  # define phi as a search criterion\n",
    "   # while phi(a,x)<obj(x-a*grad(x)):  # if f(x+a*d)>phi(a) then backtrack. d is the search direction\n",
    "    #    a = 0.5*a\n",
    "    #return a\n",
    "\n",
    "while error >= eps:  # keep searching while gradient norm is larger than eps\n",
    "    #a = line_search(x)\n",
    "    x = x - a*np.array(grad(x))\n",
    "    soln.append(x)\n",
    "    error = np.linalg.norm(grad(x))\n",
    "    \n",
    "soln  # print the search trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "893db293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0],\n",
       " array([0.0625  , 0.109375]),\n",
       " array([0.10986328, 0.19580078]),\n",
       " array([0.14542389, 0.26428223]),\n",
       " array([0.17178619, 0.31872964]),\n",
       " array([0.19098449, 0.36219818]),\n",
       " array([0.20460775, 0.39707492]),\n",
       " array([0.21389699, 0.42522498]),\n",
       " array([0.2257459 , 0.47098649]),\n",
       " array([0.22716314, 0.50022586]),\n",
       " array([0.22287655, 0.52006219]),\n",
       " array([0.20820431, 0.54894461]),\n",
       " array([0.124532  , 0.61427662]),\n",
       " array([0.08599204, 0.62803184]),\n",
       " array([0.03645422, 0.67896418]),\n",
       " array([0.02045071, 0.67844123]),\n",
       " array([-0.02277453,  0.70166208]),\n",
       " array([-0.03478701,  0.71666537]),\n",
       " array([-0.05054416,  0.72192391]),\n",
       " array([-0.06039699,  0.73242714]),\n",
       " array([-0.08354146,  0.74195478]),\n",
       " array([-0.0856678 ,  0.74706109]),\n",
       " array([-0.09917469,  0.75791006]),\n",
       " array([-0.10562305,  0.7599035 ]),\n",
       " array([-0.10953627,  0.76424141]),\n",
       " array([-0.11897805,  0.76794228]),\n",
       " array([-0.11977577,  0.77009513]),\n",
       " array([-0.12519875,  0.77452096]),\n",
       " array([-0.12784025,  0.77526882]),\n",
       " array([-0.12939171,  0.77706298]),\n",
       " array([-0.13131913,  0.77777804]),\n",
       " array([-0.13383727,  0.78031164]),\n",
       " array([-0.13462999,  0.78030584]),\n",
       " array([-0.13680127,  0.78148622]),\n",
       " array([-0.13741514,  0.78222939]),\n",
       " array([-0.13820272,  0.78250401]),\n",
       " array([-0.13870403,  0.78302604]),\n",
       " array([-0.13986305,  0.78351698]),\n",
       " array([-0.13997472,  0.78376751]),\n",
       " array([-0.14065759,  0.78431081]),\n",
       " array([-0.1409797 ,  0.78441549]),\n",
       " array([-0.14117901,  0.7846309 ]),\n",
       " array([-0.1416516 ,  0.78482215]),\n",
       " array([-0.14169378,  0.78492766]),\n",
       " array([-0.14196804,  0.78514919]),\n",
       " array([-0.14209991,  0.78518873]),\n",
       " array([-0.14217902,  0.78527775]),\n",
       " array([-0.14227544,  0.78531483]),\n",
       " array([-0.14240338,  0.78544092]),\n",
       " array([-0.14244267,  0.78544161])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code using line search-- gradinet method\n",
    "import numpy as np\n",
    "obj = lambda x: 5*x[0]**2+10*x[1]**2+(12*x[0]*x[1])-8*x[0]-14*x[1]+5# note that this is 1D. In Prob 2 it should be 2D.\n",
    "def grad(x):\n",
    "     return np.array([10*x[0]+12*x[1]-8, 20*x[1]+12*x[0]-14])\n",
    "eps = 1e-3  # termination criterion  \n",
    "x0= [0.,0.]  # initial guess\n",
    "k = 0  # counter\n",
    "soln = [x0]  # use an array to store the search steps    \n",
    "x = soln[k]  # start with the initial guess\n",
    "error = np.linalg.norm(grad(x))  # compute the error. Note you will need to compute the norm for 2D grads, rather than the absolute value\n",
    "#a = 0.01  # set a fixed step size to start with\n",
    "\n",
    "\n",
    "# Armijo line search\n",
    "def line_search(x):\n",
    "    a = 1.  # initialize step size\n",
    "    d=-grad(x)\n",
    "    phi = lambda a, x: obj(x) + a*0.8*np.dot(grad(x),d)  # define phi as a search criterion\n",
    "    while phi(a,x)<obj(x-a*grad(x)):  # if f(x+a*d)>phi(a) then backtrack. d is the search direction\n",
    "        a = 0.5*a\n",
    "    return a\n",
    "\n",
    "while error >= eps:  # keep searching while gradient norm is larger than eps\n",
    "    a = line_search(x)\n",
    "    x = x - a*np.array(grad(x))\n",
    "    soln.append(x)\n",
    "    error = np.linalg.norm(grad(x))\n",
    "    \n",
    "soln  # print the search trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7c6c056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0]),\n",
       " array([-0.03571429,  0.19642857]),\n",
       " array([-0.0625 ,  0.34375]),\n",
       " array([-0.08258929,  0.45424107]),\n",
       " array([-0.09765625,  0.53710938]),\n",
       " array([-0.10895647,  0.5992606 ]),\n",
       " array([-0.11743164,  0.64587402]),\n",
       " array([-0.12378802,  0.68083409]),\n",
       " array([-0.1285553 ,  0.70705414]),\n",
       " array([-0.13213076,  0.72671918]),\n",
       " array([-0.13481236,  0.74146795]),\n",
       " array([-0.13682355,  0.75252954]),\n",
       " array([-0.13833195,  0.76082572]),\n",
       " array([-0.13946325,  0.76704786]),\n",
       " array([-0.14031172,  0.77171447]),\n",
       " array([-0.14094808,  0.77521442]),\n",
       " array([-0.14142534,  0.77783939]),\n",
       " array([-0.14178329,  0.77980811]),\n",
       " array([-0.14205176,  0.78128466]),\n",
       " array([-0.1422531 ,  0.78239206]),\n",
       " array([-0.14240411,  0.78322262]),\n",
       " array([-0.14251737,  0.78384554]),\n",
       " array([-0.14260231,  0.78431272]),\n",
       " array([-0.14266602,  0.78466311]),\n",
       " array([-0.1427138 ,  0.78492591]),\n",
       " array([-0.14274964,  0.785123  ]),\n",
       " array([-0.14277651,  0.78527082]),\n",
       " array([-0.14279667,  0.78538169]),\n",
       " array([-0.14281179,  0.78546484]),\n",
       " array([-0.14282313,  0.7855272 ]),\n",
       " array([-0.14283163,  0.78557397]),\n",
       " array([-0.14283801,  0.78560905]),\n",
       " array([-0.14284279,  0.78563536]),\n",
       " array([-0.14284638,  0.78565509]),\n",
       " array([-0.14284907,  0.78566989])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code using line search-- newton method\n",
    "import numpy as np\n",
    "obj = lambda x: (2-2*x[0]-3*x[1])**2 + x[0]**2 +(x[1]-1)**2# note that this is 1D. In Prob 2 it should be 2D.\n",
    "def grad(x):\n",
    "     return np.array([10*x[0]+12*x[1]-8, 20*x[1]+12*x[0]-14])\n",
    "eps = 1e-3  # termination criterion  \n",
    "x0= np.zeros((2),dtype=int)  # initial guess\n",
    "k = 0  # counter\n",
    "soln = [x0]  # use an array to store the search steps    \n",
    "x = soln[k]  # start with the initial guess\n",
    "error = np.linalg.norm(grad(x))  # compute the error. Note you will need to compute the norm for 2D grads, rather than the absolute value\n",
    "a = 1  # set a fixed step size to start with\n",
    "\n",
    "# Armijo line search\n",
    "def line_search(x):\n",
    "    a = 1.  # initialize step size\n",
    "    h=([[10,12],[12,20]])\n",
    "    d=-np.matmul(np.linalg.inv(h),grad(x))\n",
    "    def phi(a,x):\n",
    "        return obj(x)+a*0.8*np.dot(grad(x),d)\n",
    "    \n",
    "#     phi = lambda a, x: obj(x) + a*0.8*np.dot(grad(x),d)  # define phi as a search criterion\n",
    "    while phi(a,x)<obj(x+a*d):  # while phi(a,x)<obj(x-a*grad(x)): # if f(x+a*d)>phi(a) then backtrack. d is the search direction\n",
    "        a = 0.5*a\n",
    "    return a\n",
    "\n",
    "while error >= eps:  # keep searching while gradient norm is larger than eps\n",
    "    a = line_search(x)\n",
    "    h=([[10,12],[12,20]])\n",
    "    d=-np.matmul(np.linalg.inv(h),grad(x))\n",
    "    x = x+a*d\n",
    "    soln.append(x)\n",
    "    error = np.linalg.norm(grad(x))\n",
    "    \n",
    "soln  # print the search trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc3060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d666aa14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
