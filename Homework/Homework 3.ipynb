{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "harmful-logging",
   "metadata": {},
   "source": [
    "### Problem 1 (50 points) \n",
    "\n",
    "Vapor-liquid equilibria data are correlated using two adjustable parameters $A_{12}$ and $A_{21}$ per binary\n",
    "mixture. For low pressures, the equilibrium relation can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p = & x_1\\exp\\left(A_{12}\\left(\\frac{A_{21}x_2}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{water}^{sat}\\\\\n",
    "& + x_2\\exp\\left(A_{21}\\left(\\frac{A_{12}x_1}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{1,4 dioxane}^{sat}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here the saturation pressures are given by the Antoine equation\n",
    "\n",
    "$$\n",
    "\\log_{10}(p^{sat}) = a_1 - \\frac{a_2}{T + a_3},\n",
    "$$\n",
    "\n",
    "where $T = 20$($^{\\circ}{\\rm C}$) and $a_{1,2,3}$ for a water - 1,4 dioxane\n",
    "system is given below.\n",
    "\n",
    "|             | $a_1$     | $a_2$      | $a_3$     |\n",
    "|:------------|:--------|:---------|:--------|\n",
    "| Water       | 8.07131 | 1730.63  | 233.426 |\n",
    "| 1,4 dioxane | 7.43155 | 1554.679 | 240.337 |\n",
    "\n",
    "\n",
    "The following table lists the measured data. Recall that in a binary system $x_1 + x_2 = 1$.\n",
    "\n",
    "|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n",
    "|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
    "|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n",
    "\n",
    "Estimate $A_{12}$ and $A_{21}$ using data from the above table: \n",
    "\n",
    "1. Formulate the least square problem; \n",
    "2. Since the model is nonlinear, the problem does not have an analytical solution. Therefore, solve it using the gradient descent or Newton's method implemented in HW1; \n",
    "3. Compare your optimized model with the data. Does your model fit well with the data?\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 2 (50 points) \n",
    "\n",
    "Solve the following problem using Bayesian Optimization:\n",
    "$$\n",
    "    \\min_{x_1, x_2} \\quad \\left(4-2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2,\n",
    "$$\n",
    "for $x_1 \\in [-3,3]$ and $x_2 \\in [-2,2]$. A tutorial on Bayesian Optimization can be found [here](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7b51354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error tensor(1714.5715)\n",
      "e tensor([ -18.5664, 1714.4711])\n",
      "after loop 2.9103830456733704e-11\n",
      "second a tensor([2., 1.], grad_fn=<AddBackward0>)\n",
      "after None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-0148071551d5>:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  soln = torch.tensor(a)\n",
      "<ipython-input-6-0148071551d5>:92: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  print(\"after\",a.grad)                     #not able to calculate grad here\n",
      "<ipython-input-6-0148071551d5>:94: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  error = torch.linalg.norm(a.grad)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linalg_norm(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0148071551d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"after\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m                     \u001b[1;31m#not able to calculate grad here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: linalg_norm(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "# calculating psat water and psat 1,4 dioxide\n",
    "import math\n",
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def line_search(a, e):\n",
    "    start_step = 1.  # initialize step size\n",
    "    \n",
    "    def phi(start_step,a,e):\n",
    "        return obj(a)+start_step*0.8*np.dot(a.grad,e)\n",
    "\n",
    "    while phi(start_step,a,e)<obj(a+start_step*e):  # while phi(a,x)<obj(x-a*grad(x)): # if f(x+a*d)>phi(a) then backtrack. d is the search direction\n",
    "        start_step = 0.5*start_step\n",
    "       # print(\"second step\",start_step)\n",
    "        \n",
    "    return start_step\n",
    "\n",
    "\n",
    "\n",
    "a = Variable(t.tensor([2.,1.]), requires_grad=True)\n",
    "\n",
    "b=torch.tensor([[8.07131,1730.63,233.426],[7.43155,1554.679,240.337]])\n",
    "#psat=torch.tensor([ ])\n",
    "lm=[]\n",
    "x1=torch.tensor([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])\n",
    "pexp=torch.tensor([28.1,34.4,36.7,36.9,36.8,36.7,36.5,35.4,32.9,27.7,17.5])\n",
    "loss=0\n",
    "    \n",
    "for i in range(2):\n",
    "    l=b[i][1]/(20+b[0][2])\n",
    "    m=b[i][0]-l\n",
    "    d=[10**m]\n",
    "    lm.append(d)\n",
    "\n",
    "psat=torch.tensor(lm)\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    x2=1-x1[i]\n",
    "    l1=a[1]*x2/(a[0]*x1[i]+a[1]*x2)\n",
    "    l1woexp=a[0]*(l1**2)    \n",
    "    l1wexp=torch.exp(l1woexp)\n",
    "    l1full=x1[i]*l1wexp*psat[0]\n",
    "    r1=a[0]*x1[i]/(a[0]*x1[i]+a[1]*x2)\n",
    "    r1woexp=a[1]*(l1**2)\n",
    "    r1wexp=torch.exp(r1woexp)\n",
    "    r1full=x1[i]*r1wexp*psat[1]  \n",
    "    pcal=l1full+r1full           \n",
    "    loss=loss+(pcal-pexp[i])**2\n",
    "\n",
    "obj = lambda a:loss\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "eps = 1e-3  \n",
    "\n",
    "\n",
    "k = 0  \n",
    "\n",
    "soln = torch.tensor(a) \n",
    "calnorm=a.grad\n",
    "\n",
    "\n",
    "\n",
    "error = torch.linalg.norm(calnorm)\n",
    "print(\"error\",error)\n",
    "                          \n",
    "start_step = 0.01 \n",
    "\n",
    "# Armijo line search\n",
    "\n",
    "\n",
    "while error >= eps:  # keep searching while gradient norm is larger than eps\n",
    "    \n",
    "    e = -a.grad\n",
    "    print(\"e\",e)\n",
    "    \n",
    "    \n",
    "    \n",
    "    start_step = line_search(a, e)\n",
    "    \n",
    "    print(\"after loop\", start_step)\n",
    "    \n",
    "\n",
    "    \n",
    "    a = a+start_step*e\n",
    "    \n",
    "    print(\"second a\",a)\n",
    "    \n",
    "    \n",
    "    print(\"after\",a.grad)                     #not able to calculate grad here     \n",
    "    \n",
    "    error = torch.linalg.norm(a.grad)\n",
    "    print(error)\n",
    "\n",
    "print(a.data.numpy())\n",
    "print(loss.data.numpy())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0bd76ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    x0     |    x1     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.265   \u001b[0m | \u001b[0m-0.4979  \u001b[0m | \u001b[0m 0.8813  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-110.1   \u001b[0m | \u001b[0m-2.999   \u001b[0m | \u001b[0m-0.7907  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.4933  \u001b[0m | \u001b[0m-0.3849  \u001b[0m | \u001b[0m 1.039   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.2833  \u001b[0m | \u001b[0m 1.599   \u001b[0m | \u001b[0m-0.5696  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-162.9   \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-47.77   \u001b[0m | \u001b[0m 0.3665  \u001b[0m | \u001b[0m-2.0     \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-150.9   \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m-2.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.7638  \u001b[0m | \u001b[0m 0.4683  \u001b[0m | \u001b[0m-0.02769 \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-48.27   \u001b[0m | \u001b[0m-2.043   \u001b[0m | \u001b[0m 2.0     \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-2.236   \u001b[0m | \u001b[0m 1.314   \u001b[0m | \u001b[0m 0.6422  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-50.26   \u001b[0m | \u001b[0m 0.5766  \u001b[0m | \u001b[0m 2.0     \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-52.97   \u001b[0m | \u001b[0m-1.299   \u001b[0m | \u001b[0m-2.0     \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-1.06    \u001b[0m | \u001b[0m-0.7025  \u001b[0m | \u001b[0m-0.5363  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-7.993   \u001b[0m | \u001b[0m 2.197   \u001b[0m | \u001b[0m 0.07143 \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-48.19   \u001b[0m | \u001b[0m-0.8016  \u001b[0m | \u001b[0m 2.0     \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-1.426   \u001b[0m | \u001b[0m-1.507   \u001b[0m | \u001b[0m 0.2863  \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-107.6   \u001b[0m | \u001b[0m-3.0     \u001b[0m | \u001b[0m 1.181   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-2.305   \u001b[0m | \u001b[0m 1.361   \u001b[0m | \u001b[0m-0.01255 \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.03572 \u001b[0m | \u001b[0m 0.4022  \u001b[0m | \u001b[0m 0.7855  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.217   \u001b[0m | \u001b[0m 0.5131  \u001b[0m | \u001b[0m-0.8858  \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-1.746   \u001b[0m | \u001b[0m-0.8282  \u001b[0m | \u001b[0m 0.09644 \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.8247  \u001b[0m | \u001b[0m-1.335   \u001b[0m | \u001b[0m 0.9693  \u001b[0m |\n",
      "| \u001b[95m 23      \u001b[0m | \u001b[95m 0.9014  \u001b[0m | \u001b[95m-0.0734  \u001b[0m | \u001b[95m-0.6431  \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.6251  \u001b[0m | \u001b[0m 1.033   \u001b[0m | \u001b[0m-0.6554  \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-3.03    \u001b[0m | \u001b[0m-0.3903  \u001b[0m | \u001b[0m-1.169   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-2.223   \u001b[0m | \u001b[0m-1.358   \u001b[0m | \u001b[0m-0.5658  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.7812  \u001b[0m | \u001b[0m-1.108   \u001b[0m | \u001b[0m 0.5979  \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.5577  \u001b[0m | \u001b[0m-0.05554 \u001b[0m | \u001b[0m 0.4046  \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-1.608   \u001b[0m | \u001b[0m 0.8407  \u001b[0m | \u001b[0m 0.4655  \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-2.496   \u001b[0m | \u001b[0m-1.242   \u001b[0m | \u001b[0m-0.1562  \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-3.177   \u001b[0m | \u001b[0m-0.9934  \u001b[0m | \u001b[0m-0.9956  \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.2506  \u001b[0m | \u001b[0m 0.4613  \u001b[0m | \u001b[0m-0.512   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-1.837   \u001b[0m | \u001b[0m 1.823   \u001b[0m | \u001b[0m-0.1931  \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.8815  \u001b[0m | \u001b[0m-0.08294 \u001b[0m | \u001b[0m 0.8368  \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.03018 \u001b[0m | \u001b[0m-0.1394  \u001b[0m | \u001b[0m-0.1282  \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-1.517   \u001b[0m | \u001b[0m 1.257   \u001b[0m | \u001b[0m-1.049   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-2.383   \u001b[0m | \u001b[0m 1.78    \u001b[0m | \u001b[0m 0.3605  \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.05348 \u001b[0m | \u001b[0m-1.581   \u001b[0m | \u001b[0m 0.7205  \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 0.3071  \u001b[0m | \u001b[0m 0.08616 \u001b[0m | \u001b[0m-0.9654  \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.1615  \u001b[0m | \u001b[0m 0.3051  \u001b[0m | \u001b[0m 0.4539  \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-0.03338 \u001b[0m | \u001b[0m-0.3842  \u001b[0m | \u001b[0m-0.8386  \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.04223 \u001b[0m | \u001b[0m-0.4333  \u001b[0m | \u001b[0m 0.4041  \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.2273  \u001b[0m | \u001b[0m 0.164   \u001b[0m | \u001b[0m-0.2792  \u001b[0m |\n",
      "| \u001b[95m 44      \u001b[0m | \u001b[95m 0.9568  \u001b[0m | \u001b[95m 0.2315  \u001b[0m | \u001b[95m-0.7249  \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-0.4108  \u001b[0m | \u001b[0m-1.398   \u001b[0m | \u001b[0m 0.6454  \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-0.3415  \u001b[0m | \u001b[0m 1.342   \u001b[0m | \u001b[0m-0.7692  \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-1.076   \u001b[0m | \u001b[0m 0.9523  \u001b[0m | \u001b[0m-0.982   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-1.941   \u001b[0m | \u001b[0m 0.8296  \u001b[0m | \u001b[0m 0.8889  \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-0.8011  \u001b[0m | \u001b[0m 0.1395  \u001b[0m | \u001b[0m 1.062   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-0.6696  \u001b[0m | \u001b[0m-0.848   \u001b[0m | \u001b[0m 0.9324  \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-0.01326 \u001b[0m | \u001b[0m 0.1221  \u001b[0m | \u001b[0m 0.1245  \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-1.996   \u001b[0m | \u001b[0m-1.044   \u001b[0m | \u001b[0m-0.6295  \u001b[0m |\n",
      "=================================================\n",
      "{'target': 0.9568135408880579, 'params': {'x0': 0.23151603129184473, 'x1': -0.7248785139747396}}\n"
     ]
    }
   ],
   "source": [
    " #!pip install bayesian-optimization\n",
    "    \n",
    "from bayes_opt import BayesianOptimization\n",
    "def fun(x0,x1):\n",
    "    return -((4-(2.1*x0**2)+((x0**4)/3))*x0**2+(x0*x1)+(-4+(4*x1**2))*x1**2)\n",
    "    from bayes_opt import BayesianOptimization\n",
    "limits= {'x0': (-3,3), 'x1': (-2,2)}\n",
    "\n",
    "optimizer= BayesianOptimization(f=fun, pbounds=limits, random_state=1)\n",
    "optimizer.maximize(init_points=2,n_iter=50)\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95f722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
